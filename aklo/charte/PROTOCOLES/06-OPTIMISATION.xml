<?xml version="1.0" encoding="UTF-8"?>
<protocol name="performance_optimization">
  <description>
    This protocol is activated when a feature does not meet its performance requirements (response time, memory/CPU consumption). The golden rule is: **Measure first, then optimize.**
  </description>

  <section title="Mission and Deliverables">
    <sub_section title="Mission">
      <introduction>
        To scientifically identify performance bottlenecks, implement a targeted solution, and prove its effectiveness through comparative measurements.
      </introduction>
    </sub_section>
    <sub_section title="Expected Deliverables">
      <deliverables>
        <deliverable type="report">
          A complete `OPTIM-[ID]-DONE.xml` file, created in `/docs/backlog/06-optim/`.
        </deliverable>
        <deliverable type="commit">
          A commit containing the optimized code, produced via the [DEVELOPMENT] protocol.
        </deliverable>
      </deliverables>
    </sub_section>
  </section>

  <section title="Optimization Artefact Management and Structure">
    <sub_section title="Naming Convention">
      <format>OPTIM-[ID]-[Status].xml</format>
      <details>
        <detail name="id">A unique identifier generated from the title and date (e.g., `optimize-user-api-20250708`).</detail>
        <detail name="status">The status of the optimization process.</detail>
      </details>
    </sub_section>
    <sub_section title="Statuses">
      <list>
        <item name="benchmarking">Initial analysis and performance measurement are in progress.</item>
        <item name="awaiting_fix">The bottleneck is identified, and a solution is awaiting validation by the Human_Developer.</item>
        <item name="done">The optimization has been implemented, validated by a new measurement, and the commit has been created.</item>
      </list>
    </sub_section>
    <sub_section title="Mandatory Artefact Structure">
      <artefact_template format="xml">
        <optimization_report id="[id]" status="benchmarking" title="[Optimization Goal]">
          <metadata>
            <associated_task optional="true">TASK-[ID]</associated_task>
          </metadata>
          <content>
            <performance_goal>
              <metric_targeted>e.g., "API response time for GET /users".</metric_targeted>
              <quantified_objective>e.g., "Must be less than 100ms at the 95th percentile."</quantified_objective>
            </performance_goal>
            <benchmark_protocol>
              <tool_used>e.g., `k6`, `JMeter`, `console.time`.</tool_used>
              <test_scenario>A precise and reproducible description of how the measurement is performed.</test_scenario>
            </benchmark_protocol>
            <initial_measurements>
              <raw_results><![CDATA[Raw results from the benchmark tool.]]></raw_results>
              <bottleneck_analysis>Identification of the exact line, query, or function causing the slowness.</bottleneck_analysis>
            </initial_measurements>
            <optimization_strategy>
              <proposed_solution>e.g., "Cache the result with Redis for 5 minutes", "Add an index on column X of table Y".</proposed_solution>
            </optimization_strategy>
            <non_regression_proof>
              <functional_test_coverage>Confirmation that the business logic is covered by tests BEFORE starting optimization.</functional_test_coverage>
              <performance_regression_tests optional="true">Description of specific performance regression tests written or updated.</performance_regression_tests>
              <final_test_result>Assertion that 100% of functional tests pass after optimization.</final_test_result>
            </non_regression_proof>
            <final_measurements>
              <raw_results><![CDATA[Raw results from the new benchmark, run under identical conditions.]]></raw_results>
              <conclusion>Before/after comparison and confirmation that the objective has been met.</conclusion>
            </final_measurements>
          </content>
        </optimization_report>
      </artefact_template>
      <example>
        <optimization_report id="optimize-get-products-api-20250708" status="done" title="Optimize GET /api/products endpoint">
          <metadata>
            <associated_task>TASK-115-1</associated_task>
          </metadata>
          <content>
            <performance_goal>
              <metric_targeted>API response time for GET /api/products</metric_targeted>
              <quantified_objective>Average response time must be under 200ms.</quantified_objective>
            </performance_goal>
            <benchmark_protocol>
              <tool_used>k6</tool_used>
              <test_scenario>Run a **load test** with 50 virtual users for 1 minute against the GET /api/products endpoint.</test_scenario>
            </benchmark_protocol>
            <initial_measurements>
              <raw_results><![CDATA[
http_req_duration..................: avg=850.12ms min=750ms med=840ms max=1.2s p(95)=1.1s
              ]]></raw_results>
              <bottleneck_analysis>Analysis with a database profiler shows N+1 queries. The code fetches each product's category in a separate database query inside a loop.</bottleneck_analysis>
            </initial_measurements>
            <optimization_strategy>
              <proposed_solution>Refactor the database query to use a `JOIN` to fetch products and their categories in a single query.</proposed_solution>
            </optimization_strategy>
            <non_regression_proof>
              <functional_test_coverage>The endpoint is covered by 5 **integration tests** ensuring correct data format and pagination.</functional_test_coverage>
              <performance_regression_tests>A new **unit test** was added to the data access layer to assert that fetching 10 products results in exactly 1 database query.</performance_regression_tests>
              <final_test_result>All 5 integration tests and the new performance regression test pass after the code change.</final_test_result>
            </non_regression_proof>
            <final_measurements>
              <raw_results><![CDATA[
http_req_duration..................: avg=95.45ms min=80ms med=92ms max=150ms p(95)=145ms
              ]]></raw_results>
              <conclusion>The average response time was reduced from 850ms to 95ms. The objective is met.</conclusion>
            </final_measurements>
          </content>
        </optimization_report>
      </example>
    </sub_section>
  </section>

  <section title="Optimization Procedure">
    <flight_plan required="true" name="OPTIMIZATION_FLIGHT_PLAN">
      <objective>Optimize performance based on measurable and objective metrics.</objective>
      <actions>
        <item>Generate a unique ID for the optimization report.</item>
        <item>Create the `OPTIM-[ID]-BENCHMARKING.xml` file.</item>
        <item>Define the performance goal with a quantified metric.</item>
        <item>Establish a reproducible benchmark protocol.</item>
        <item>Measure initial performance before optimization.</item>
        <item>Perform scientific analysis to identify the bottleneck.</item>
        <item>Propose a targeted optimization strategy.</item>
        <item>Implement via the [DEVELOPMENT] protocol after validation.</item>
        <item>Measure final performance to prove the improvement.</item>
      </actions>
      <affected_files>
        <file type="lifecycle">/docs/backlog/06-optim/OPTIM-[ID].xml (BENCHMARKING → AWAITING_FIX → DONE)</file>
        <file type="modification">Source code files to be optimized.</file>
        <file type="creation">Benchmark scripts or configurations.</file>
      </affected_files>
      <system_commands>
        <command optional="true">`aklo optimize "[Title]"`</command>
        <command>Benchmark tools (k6, JMeter, etc.).</command>
        <command>Profiling and performance analysis tools.</command>
      </system_commands>
      <mcp_tools>
        <tool>mcp_desktop-commander_write_file</tool>
        <tool>mcp_desktop-commander_execute_command</tool>
        <tool>mcp_desktop-commander_edit_block</tool>
      </mcp_tools>
      <required_validation>YES - Explicit approval required before optimization.</required_validation>
    </flight_plan>

    <step number="1" type="procedure" title="Measure (Non-negotiable)">
      <action>Create an `OPTIM-[ID]-BENCHMARKING.xml` file. Fill in the "Performance Goal" and "Benchmark Protocol" sections. Execute the benchmark on the current code and fill in the "Initial Measurements" section.</action>
      <automation optional="true">`aklo optimize "[Optimization Title]"`</automation>
    </step>
    <step number="2" type="analysis" title="Analyze and Propose">
      <action>Analyze the results to identify the bottleneck with certainty. Fill in the "Optimization Strategy" section with a targeted correction proposal. Change the status to `AWAITING_FIX`.</action>
      <awaiting_validation>Submit the report and solution proposal to the Human_Developer for validation.</awaiting_validation>
    </step>
    <step number="3" type="procedure" title="Implement and Validate">
      <action>Once the plan is approved, activate the [DEVELOPMENT] protocol to implement the solution. After implementation, rerun the exact same benchmark as in Phase 1. Fill in the "Final Measurements" section of the report to prove the improvement.</action>
    </step>
    <step number="4" type="conclusion" title="Finalization">
      <action>After confirming the performance improvement, fill in the "Non-Regression Proof" section. Follow Step 6 (Pre-Commit Review) of the [DEVELOPMENT] protocol. This review may formally activate the **[CODE REVIEW]** protocol if changes are complex. Once approved, follow Step 7 of the [DEVELOPMENT] protocol to update the optimization report status to `DONE`, consider activating the [KNOWLEDGE-BASE] protocol, and create the final commit.</action>
    </step>
  </section>
  
  <section title="Commit Management">
    <rule>
      The final commit containing the optimization is produced by the [DEVELOPMENT] protocol, which is activated in Phase 3. This ensures all code changes adhere to the same quality, testing, and validation pipeline.
    </rule>
  </section>
</protocol>